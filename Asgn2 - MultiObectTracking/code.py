# -*- coding: utf-8 -*-
"""HW4_Q1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AsxHIaTsBLUS1BOl5jJTXc5wdtUcFncx

## **CV HW4: Multi-object Tracking (MOT) with Detection**
**Detection**: YOLOv5, 
**Tracking**: Simple Online Realtime Tracking (SORT)

---
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **1. Unzip data folder**"""

# Change the path according to your setup 
!unzip '/content/drive/MyDrive/hw4/sort-master.zip'
!unzip '/content/drive/MyDrive/hw4/KITTI_17_images.zip'

"""# **2. Install requirements**"""

!pip install -r sort-master/requirements.txt
!pip install cv
!pip install filterpy

"""# **3. Import libraries**"""

import torch
import torchvision
import cv2
import sys
import filterpy
sys.path.insert(0,'./sort-master/')
import matplotlib
from google.colab.patches import cv2_imshow
from collections import namedtuple, OrderedDict

"""# **4. Load YOLOv5 detector from torch hub**"""

yolov5_detector = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained = True)
yolov5_detector.float()
yolov5_detector.eval()

"""# **5. Import SORT library**"""

from sort import *

"""#**6. Perform tracking with detection**"""

import cv2
import torch
import numpy as np
import os
import glob
from google.colab.patches import cv2_imshow

out_bboxes=[]

# Initialize SORT tracker
tracker = Sort()

# Read frames from directory of images
frame_dir = '/content/KITTI_17_images'
frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.jpg')))

# Define output codec and video writer object
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
frame_size= (1280,720)
fps= 30
out = cv2.VideoWriter('output_video.mp4', fourcc, fps, frame_size)

for frame_idx,frame_path in enumerate(frame_paths):
  # Read frame from file
  frame = cv2.imread(frame_path)

  # Detect objects using YOLOv5s
  results = yolov5_detector(frame)
  results_df = results.pandas().xyxy[0]
  # print(res)

  # Extract bounding boxes and class labels
  bboxes = []
  for i in range(len(results_df)):
      if results_df.loc[i,"class"] == 0:  # Only track pedestrians (class ID 0)
        out_bboxes.append([frame_idx+1])
        coords = np.array(results_df.iloc[i,:4])
        confidence = results_df.iloc[i,4]
        bboxes.append(coords)
        out_bboxes[-1].append(results_df.loc[i,"class"])
        for coord in coords:
          out_bboxes[-1].append(coord)
        out_bboxes[-1].append(confidence)
        out_bboxes[-1].append(-1)
        out_bboxes[-1].append(-1)
        out_bboxes[-1].append(-1)
        

  # Perform tracking with SORT
  if bboxes:
      bboxes = np.array(bboxes)
      dets = np.hstack((bboxes, np.ones((bboxes.shape[0], 1))))
      tracks = tracker.update(dets)

      # Draw bounding boxes and track IDs on the frame
      for track in tracks:
          bbox = track[:4].astype(np.int32)
          track_id = int(track[4])
          cv2.rectangle(frame, tuple(bbox[:2]), tuple(bbox[2:]), (0, 255, 0), 2)
          cv2.putText(frame, str(track_id), (bbox[0], bbox[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

  # Resize frame to match video frame size if necessary
  if frame.shape[:2] != frame_size:
      frame = cv2.resize(frame, frame_size)
  # Write frame with tracking results to output video
  out.write(frame)

  # Display the resulting frame
  cv2_imshow(frame)

# Release resources
out.release()
cv2.destroyAllWindows()

with open("output.txt", "w") as f:
    for row in out_bboxes:
        row_str = ",".join(str(elem) for elem in row)
        f.write("%s\n " % row_str)

results_df

"""# **7. Report Evaluation Metrics**"""

gt_boxes = []
with open('/content/drive/MyDrive/hw4/gt.txt', 'r') as f:
  for line in f.readlines():
    gt_boxes.append([float(x) for x in line.split(",")])

!pip install motmetrics

import motmetrics as mm
import numpy as np

gt_bboxes = np.array(gt_boxes)
pred_bboxes = np.array(out_bboxes)
# Define the ground truth and predicted bounding boxes
# in the format [frame_id, object_id, x1, y1, x2, y2, confidence]
# gt_bboxes = np.array([
#     [0, 0, 10, 20, 50, 80],
#     [0, 1, 80, 60, 120, 100],
#     [1, 0, 15, 25, 60, 85],
#     [1, 1, 85, 65, 130, 110]
# ])
# pred_bboxes = np.array([
#     [0, 0, 15, 20, 45, 80, 0.9],
#     [0, 1, 80, 60, 120, 100, 0.8],
#     [1, 0, 20, 25, 70, 80, 0.7],
#     [1, 1, 85, 65, 130, 110, 0.85]
# ])

# Define the distance metric to use (we'll use the IoU distance)
metric = mm.distances.iou_matrix

# Compute the distance matrix between the ground truth and predicted bboxes
dist_matrix = metric(gt_bboxes[:, 2:6], pred_bboxes[:, 2:6])

# Compute the matches and corresponding costs using the Hungarian algorithm
matches, costs = mm.lap.lsa_solve(-dist_matrix)

# Compute the ID switches and missed and false detections
tracker = mm.Tracktor(metric='euclidean', max_lost=3, tracker_output=False)
acc = mm.MOTAccumulator(auto_id=True)
for i in range(len(gt_bboxes)):
    acc.update(gt_bboxes[i, :6], pred_bboxes[i, :6], dist_matrix[i])

switches = acc.IDSwitches
misses = acc.Misses
false_alarms = acc.FalseAlarms

# Compute the MOTA and MOTP
mota = mm.utils.mot.aMOTSA(switches, misses, false_alarms)
motp = 1 - np.mean(costs[matches[:, 0], matches[:, 1]])

# Print the results
print(f'MOTA: {mota:.2f}')
print(f'MOTP: {motp:.2f}')